# Controller/ContinuousLearningController.py
from PySide6.QtCore import QObject, Signal, Slot, Property, QThread, QMutex
from PySide6.QtWidgets import QVBoxLayout, QFileDialog
from typing import Dict, Any, List
import logging
import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
import joblib
from matplotlib.figure import Figure
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import json
import time
import sys

# å¯¼å…¥æ•°æ®å¤„ç†å™¨
from .DataProcessor import DataProcessor

# å¯¼å…¥é‡æ„åçš„ç»Ÿä¸€é¢„æµ‹å™¨æ¥å£
import sys
sys.path.append(str(Path(__file__).parent.parent))


from models.model import (
    BasePredictor, GLRPredictor, QFPredictor, TDHPredictor,
    TrainingConfig, ModelInfo, CallbackEvent, CallbackData,
    BaseCallback, MetricsCallback, ProgressCallback, LossCallback,
    GLRInput, QFInput, SVRInput
)


from models.ModelFeatureConfig import ModelFeatureConfig


logger = logging.getLogger(__name__)


class ThreadProgressCallback(BaseCallback):
    """çº¿ç¨‹è¿›åº¦å›è°ƒ - å°†é¢„æµ‹å™¨çš„å›è°ƒè½¬å‘åˆ°çº¿ç¨‹ä¿¡å·"""
    
    def __init__(self, thread):
        self.thread = thread
        
    def on_progress_update(self, callback_data: CallbackData):
        progress = callback_data.get('progress', 0)
        status = callback_data.get('status', 'è®­ç»ƒä¸­...')
        self.thread.trainingProgressUpdated.emit(progress, {"status": status})
        
    def on_loss_update(self, callback_data: CallbackData):
        self.thread.lossDataUpdated.emit(callback_data.data)
        
    def on_epoch_end(self, callback_data: CallbackData):
        epoch = callback_data.get('epoch', 0)
        loss = callback_data.get('train_loss', 0)
        val_loss = callback_data.get('val_loss', 0)
        
        # å‘é€è®­ç»ƒæ—¥å¿—
        log_msg = f"Epoch {epoch}: train_loss={loss:.4f}, val_loss={val_loss:.4f}"
        self.thread.trainingLogUpdated.emit(log_msg)


class ThreadMetricsCallback(MetricsCallback):
    """çº¿ç¨‹æŒ‡æ ‡å›è°ƒ - è®¡ç®—å¹¶è½¬å‘è®­ç»ƒæŒ‡æ ‡"""
    
    def __init__(self, thread, metrics_funcs=None):
        super().__init__(metrics_funcs)
        self.thread = thread
        
    def on_epoch_end(self, callback_data: CallbackData):
        super().on_epoch_end(callback_data)
        
        # è½¬å‘åˆ°çº¿ç¨‹çš„æ—¥å¿—ä¿¡å·
        if self.metrics_history:
            latest_metrics = self.metrics_history[-1]['metrics']
            for name, value in latest_metrics.items():
                if value is not None:
                    log_msg = f"Metric {name}: {value:.4f}"
                    self.thread.trainingLogUpdated.emit(log_msg)


class ModelTrainingThread(QThread):
    """æ¨¡å‹è®­ç»ƒçº¿ç¨‹ç±» - é‡æ„ä¸ºä½¿ç”¨ç»Ÿä¸€é¢„æµ‹å™¨æ¥å£"""
    
    # è®­ç»ƒçº¿ç¨‹ä¿¡å·
    trainingProgressUpdated = Signal(float, dict)
    trainingCompleted = Signal(str, dict)
    trainingError = Signal(str)
    trainingLogUpdated = Signal(str)
    lossDataUpdated = Signal(dict)
    
    def __init__(self, project_id, table_names, features, target_label, task_type, 
                 db_path, feature_mapping=None, training_params=None):
        super().__init__()
        self.project_id = project_id
        self.table_names = table_names
        self.features = features
        self.target_label = target_label
        self.task_type = task_type
        self.db_path = db_path
        self.feature_mapping = feature_mapping or {}
        self.training_params = training_params or {}
        self.model_info = None
        self.model_name = None
        self.predictor = None
        
        # è®¾ç½®æ—¥å¿—æ•è·
        self.setup_log_capture()
    
    def setup_log_capture(self):
        """è®¾ç½®æ—¥å¿—æ•è·"""
        import logging
        from loguru import logger
        
        class LogHandler(logging.Handler):
            def __init__(self, signal_emitter):
                super().__init__()
                self.signal_emitter = signal_emitter
                
            def emit(self, record):
                log_entry = self.format(record)
                if self.signal_emitter:
                    self.signal_emitter.trainingLogUpdated.emit(log_entry)
        
        # åˆ›å»ºå¤„ç†å™¨å¹¶æ·»åŠ åˆ°logger
        self.log_handler = LogHandler(self)
        self.log_handler.setFormatter(logging.Formatter(
            '%(asctime)s | %(levelname)s | %(name)s:%(funcName)s:%(lineno)d - %(message)s'
        ))
        
        # æ·»åŠ åˆ°Pythonæ ‡å‡†logging
        root_logger = logging.getLogger()
        root_logger.addHandler(self.log_handler)
    
    def cleanup_log_capture(self):
        """æ¸…ç†æ—¥å¿—æ•è·"""
        if hasattr(self, 'log_handler') and self.log_handler:
            logging.getLogger().removeHandler(self.log_handler)
            self.log_handler = None
    
    def run(self):
        """æ‰§è¡Œè®­ç»ƒä»»åŠ¡ - ä½¿ç”¨ç»Ÿä¸€é¢„æµ‹å™¨æ¥å£"""
        try:
            # åŠ è½½å’Œåˆå¹¶æ•°æ®
            self.trainingLogUpdated.emit("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...")
            X, y, cleaning_info = self._load_and_prepare_data()
            
            if X is None or y is None:
                self.trainingError.emit("æ•°æ®å‡†å¤‡å¤±è´¥")
                return
            
            self.trainingProgressUpdated.emit(20.0, {"status": "æ•°æ®å‡†å¤‡å®Œæˆ"})
            
            # åˆ›å»ºè®­ç»ƒé…ç½®
            config = TrainingConfig(
                epochs=self.training_params.get('epochs', 1000),
                batch_size=self.training_params.get('batch_size', 48),
                learning_rate=self.training_params.get('learning_rate', 0.001),
                patience=self.training_params.get('patience', 100),
                test_size=0.2,
                random_state=42
            )
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºé¢„æµ‹å™¨
            self.trainingLogUpdated.emit(f"åˆ›å»º{self.task_type}é¢„æµ‹å™¨...")
            self.predictor = self._create_predictor(X, y, config)
            
            if self.predictor is None:
                self.trainingError.emit(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {self.task_type}")
                return
            
            # æ·»åŠ å›è°ƒå‡½æ•°
            self._setup_callbacks()
            
            self.trainingProgressUpdated.emit(40.0, {"status": f"å¼€å§‹{self.task_type}æ¨¡å‹è®­ç»ƒ"})
            
            # æ‰§è¡Œè®­ç»ƒ
            self.trainingLogUpdated.emit("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            train_result = self.predictor.train()
            
            # è°ƒè¯•ï¼šæ£€æŸ¥è®­ç»ƒåçš„é¢„æµ‹å™¨çŠ¶æ€
            self.trainingLogUpdated.emit(f"è®­ç»ƒå®Œæˆï¼Œæ£€æŸ¥é¢„æµ‹å™¨çŠ¶æ€:")
            self.trainingLogUpdated.emit(f"  - æ˜¯å¦å·²è®­ç»ƒ: {getattr(self.predictor, 'is_trained', False)}")
            self.trainingLogUpdated.emit(f"  - æ˜¯å¦æœ‰X_train: {hasattr(self.predictor, 'X_train')}")
            self.trainingLogUpdated.emit(f"  - æ˜¯å¦æœ‰y_train: {hasattr(self.predictor, 'y_train')}")
            self.trainingLogUpdated.emit(f"  - æ˜¯å¦æœ‰X_test: {hasattr(self.predictor, 'X_test')}")
            self.trainingLogUpdated.emit(f"  - æ˜¯å¦æœ‰y_test: {hasattr(self.predictor, 'y_test')}")
            
            if hasattr(self.predictor, 'X_train'):
                self.trainingLogUpdated.emit(f"  - X_train shape: {self.predictor.X_train.shape}")
            if hasattr(self.predictor, 'y_train'):
                self.trainingLogUpdated.emit(f"  - y_train shape: {self.predictor.y_train.shape}")
            if hasattr(self.predictor, 'X_test'):
                self.trainingLogUpdated.emit(f"  - X_test shape: {self.predictor.X_test.shape}")
            if hasattr(self.predictor, 'y_test'):
                self.trainingLogUpdated.emit(f"  - y_test shape: {self.predictor.y_test.shape}")
            
            self.trainingProgressUpdated.emit(80.0, {"status": "è®¡ç®—è¯„ä¼°æŒ‡æ ‡"})
            
            # è·å–æµ‹è¯•ç»“æœ
            self.trainingLogUpdated.emit("å¼€å§‹æµ‹è¯•æ¨¡å‹...")
            test_result = self.predictor.test()
            
            # è°ƒè¯•ï¼šæ£€æŸ¥æµ‹è¯•ç»“æœ
            self.trainingLogUpdated.emit(f"æµ‹è¯•ç»“æœç±»å‹: {type(test_result)}")
            self.trainingLogUpdated.emit(f"æµ‹è¯•ç»“æœé”®: {list(test_result.keys()) if isinstance(test_result, dict) else 'Not a dict'}")
            if 'y_true' in test_result:
                y_true = test_result['y_true']
                self.trainingLogUpdated.emit(f"y_trueç±»å‹: {type(y_true)}, é•¿åº¦: {len(y_true) if hasattr(y_true, '__len__') else 'No length'}")
            if 'y_pred' in test_result:
                y_pred = test_result['y_pred']
                self.trainingLogUpdated.emit(f"y_predç±»å‹: {type(y_pred)}, é•¿åº¦: {len(y_pred) if hasattr(y_pred, '__len__') else 'No length'}")
            
            # å‡†å¤‡ç»“æœæ•°æ®
            result_data = self._prepare_training_results(train_result, test_result)
            
            # ç”Ÿæˆæ¨¡å‹åç§°å’Œä¿å­˜æ¨¡å‹ä¿¡æ¯
            self.model_name = self._generate_model_name()
            self._save_model_info(result_data)
            
            self.trainingProgressUpdated.emit(100.0, {"status": "è®­ç»ƒå®Œæˆ"})
            self.trainingCompleted.emit(self.model_name, result_data)
            
        except Exception as e:
            error_msg = f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}"
            logger.exception(error_msg)
            self.trainingError.emit(error_msg)
        finally:
            self.cleanup_log_capture()
    
    def _load_and_prepare_data(self):
        """åŠ è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®"""
        try:
            # åˆå¹¶å¤šä¸ªè¡¨çš„æ•°æ®
            all_dfs = []
            conn = sqlite3.connect(self.db_path)
            
            for table_name in self.table_names:
                try:
                    df = pd.read_sql_query(f"SELECT * FROM \"{table_name}\"", conn)
                    if not df.empty:
                        df['data_source'] = table_name
                        all_dfs.append(df)
                        self.trainingLogUpdated.emit(f"ä»è¡¨ {table_name} åŠ è½½äº† {len(df)} æ¡è®°å½•")
                except Exception as e:
                    self.trainingLogUpdated.emit(f"åŠ è½½è¡¨ {table_name} å¤±è´¥: {e}")
                    continue
            
            conn.close()
            
            if not all_dfs:
                self.trainingError.emit("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¡¨å¯ç”¨äºè®­ç»ƒ")
                return None, None, None
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            df = pd.concat(all_dfs, ignore_index=True)
            self.trainingLogUpdated.emit(f"æ€»å…±åˆå¹¶äº† {len(df)} æ¡è®°å½•")
            
            # åº”ç”¨ç‰¹å¾æ˜ å°„
            mapped_features = self._apply_feature_mapping()
            
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
            required_cols = mapped_features + [self.target_label]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                self.trainingError.emit(f"æ•°æ®è¡¨ä¸­ç¼ºå°‘å­—æ®µ: {missing_cols}")
                return None, None, None
            
            # ä½¿ç”¨æ•°æ®å¤„ç†å™¨æ¸…ç†æ•°æ®
            data_processor = DataProcessor(remove_outliers=True, outlier_factor=1.5)
            X, y, cleaning_info = data_processor.clean_data(df, mapped_features, self.target_label)
            
            # è®°å½•æ¸…ç†ä¿¡æ¯
            self.trainingLogUpdated.emit(f"æ•°æ®æ¸…ç†ä¿¡æ¯: {cleaning_info}")
            for step in cleaning_info["cleaning_steps"]:
                self.trainingLogUpdated.emit(f"  - {step}")
            
            # è¯¦ç»†è®°å½•æ¸…ç†åçš„æ•°æ®çŠ¶æ€
            self.trainingLogUpdated.emit(f"æ¸…ç†åæ•°æ®çŠ¶æ€:")
            self.trainingLogUpdated.emit(f"  - Xç±»å‹: {type(X)}, å½¢çŠ¶: {X.shape if hasattr(X, 'shape') else 'æ— shapeå±æ€§'}")
            self.trainingLogUpdated.emit(f"  - yç±»å‹: {type(y)}, å½¢çŠ¶: {y.shape if hasattr(y, 'shape') else 'æ— shapeå±æ€§'}")
            self.trainingLogUpdated.emit(f"  - Xå‰5è¡Œæ•°æ®: {X[:5].tolist() if hasattr(X, 'tolist') else str(X)[:200]}")
            self.trainingLogUpdated.emit(f"  - yå‰5ä¸ªå€¼: {y[:5].tolist() if hasattr(y, 'tolist') else str(y)[:200]}")
                
            if cleaning_info["final_count"] < 2:
                self.trainingError.emit(f"æ•°æ®æ¸…ç†åæ ·æœ¬æ•°é‡ä¸è¶³({cleaning_info['final_count']})ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
                return None, None, None
            
            # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•åˆ†å‰²
            min_test_samples = max(1, int(cleaning_info["final_count"] * 0.2))  # è‡³å°‘1ä¸ªæµ‹è¯•æ ·æœ¬
            if cleaning_info["final_count"] < 4:  # è‡³å°‘éœ€è¦4ä¸ªæ ·æœ¬æ‰èƒ½åˆ†å‰²æˆè®­ç»ƒå’Œæµ‹è¯•é›†
                self.trainingError.emit(f"æ•°æ®æ ·æœ¬è¿‡å°‘({cleaning_info['final_count']})ï¼Œè‡³å°‘éœ€è¦4ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")
                return None, None, None
            
            self.trainingLogUpdated.emit(f"æ•°æ®æ¸…ç†æˆåŠŸï¼Œå°†ç”¨äºè®­ç»ƒçš„æ ·æœ¬æ•°: {cleaning_info['final_count']}")
            
            return X, y, cleaning_info
            
        except Exception as e:
            self.trainingLogUpdated.emit(f"æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}")
            return None, None, None
    
    def _apply_feature_mapping(self):
        """åº”ç”¨ç‰¹å¾æ˜ å°„"""
        mapped_features = []
        if self.feature_mapping:
            self.trainingLogUpdated.emit("åº”ç”¨ç‰¹å¾æ˜ å°„:")
            for model_feature, user_feature in self.feature_mapping.items():
                if user_feature and user_feature.strip():
                    mapped_features.append(user_feature)
                    self.trainingLogUpdated.emit(f"  æ¨¡å‹ç‰¹å¾ {model_feature} â†’ ç”¨æˆ·æ•°æ® {user_feature}")
            
            if not mapped_features:
                self.trainingLogUpdated.emit("ç‰¹å¾æ˜ å°„ä¸ºç©ºï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç‰¹å¾")
                mapped_features = self.features
        else:
            mapped_features = self.features
            self.trainingLogUpdated.emit("æœªä½¿ç”¨ç‰¹å¾æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç‰¹å¾")
        
        # ğŸ”¥ æ–°å¢ï¼šè®°å½•æœ€ç»ˆç‰¹å¾é¡ºåº
        self.trainingLogUpdated.emit(f"è®­ç»ƒæ—¶æœ€ç»ˆç‰¹å¾é¡ºåº: {mapped_features}")
        return mapped_features
    
    def _create_predictor(self, X, y, config):
        """æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºé¢„æµ‹å™¨"""
        try:
            if self.task_type == "head":
                # TDHé¢„æµ‹ä»»åŠ¡
                self.trainingLogUpdated.emit("åˆ›å»ºTDHé¢„æµ‹å™¨...")
                return TDHPredictor(X, y, config)
                
            elif self.task_type == "production":
                # QFäº§é‡é¢„æµ‹ä»»åŠ¡
                self.trainingLogUpdated.emit("åˆ›å»ºQFé¢„æµ‹å™¨...")
                return QFPredictor(X, y, config)
                
            elif self.task_type == "glr":
                # GLRæ°”æ¶²æ¯”é¢„æµ‹ä»»åŠ¡
                self.trainingLogUpdated.emit("åˆ›å»ºGLRé¢„æµ‹å™¨...")
                self.trainingLogUpdated.emit(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {X.shape}")
                self.trainingLogUpdated.emit("æ³¨æ„: GLRæ¨¡å‹å°†è‡ªåŠ¨åº”ç”¨å¤šé¡¹å¼ç‰¹å¾å˜æ¢ï¼Œç‰¹å¾ç»´åº¦ä¼šä»9æ‰©å±•åˆ°54")
                return GLRPredictor(X, y, config)
                
            else:
                self.trainingLogUpdated.emit(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {self.task_type}")
                return None
                
        except Exception as e:
            self.trainingLogUpdated.emit(f"åˆ›å»ºé¢„æµ‹å™¨å¤±è´¥: {str(e)}")
            logger.exception(f"åˆ›å»ºé¢„æµ‹å™¨å¼‚å¸¸: {e}")
            return None
    
    def _setup_callbacks(self):
        """è®¾ç½®å›è°ƒå‡½æ•°"""
        # è¿›åº¦å›è°ƒ
        progress_callback = ThreadProgressCallback(self)
        self.predictor.add_callback(progress_callback)
        
        # æŒ‡æ ‡å›è°ƒ
        metrics_funcs = {
            'r2_score': lambda y_true, y_pred: 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2),
            'mse': lambda y_true, y_pred: np.mean((y_true - y_pred)**2),
            'mae': lambda y_true, y_pred: np.mean(np.abs(y_true - y_pred))
        }
        metrics_callback = ThreadMetricsCallback(self, metrics_funcs)
        self.predictor.add_callback(metrics_callback)
    
    def _prepare_training_results(self, train_result, test_result):
        """å‡†å¤‡è®­ç»ƒç»“æœæ•°æ®"""
        # æå–è®­ç»ƒå’Œæµ‹è¯•æŒ‡æ ‡
        train_metrics = train_result.get('train_metrics', {})
        test_metrics = train_result.get('test_metrics', {})
        
        self.trainingLogUpdated.emit(f"å‡†å¤‡è®­ç»ƒç»“æœæ•°æ®:")
        self.trainingLogUpdated.emit(f"  - è®­ç»ƒæŒ‡æ ‡: {list(train_metrics.keys())}")
        self.trainingLogUpdated.emit(f"  - æµ‹è¯•æŒ‡æ ‡: {list(test_metrics.keys())}")
    
        # ç”Ÿæˆç»˜å›¾æ•°æ®
        self.trainingLogUpdated.emit("å¼€å§‹ç”Ÿæˆç»˜å›¾æ•°æ®...")
        plot_data = self._generate_plot_data(test_result)
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        feature_importance = self._calculate_feature_importance()
        
        result = {
            "model_name": self.model_name,
            "model_type": self.predictor.model_info.model_type.upper(),
            "task_type": self.task_type,
            "table_names": self.table_names,
            "features": self.features,
            "target": self.target_label,
            "feature_mapping": self.feature_mapping,
            
            # è®­ç»ƒæŒ‡æ ‡
            "train_mape": train_metrics.get('mape', 0),
            "train_r2": train_metrics.get('r2', 0),
            "train_mse": train_metrics.get('mse', 0),
            "train_mae": train_metrics.get('mae', 0),
            
            # æµ‹è¯•æŒ‡æ ‡
            "test_mape": test_metrics.get('mape', 0),
            "test_r2": test_metrics.get('r2', 0),
            "test_mse": test_metrics.get('mse', 0),
            "test_mae": test_metrics.get('mae', 0),
            
            # ç»˜å›¾æ•°æ®
            "r2_plot_data": plot_data,
            "error_plot_data": plot_data,
            
            # å…¶ä»–ä¿¡æ¯
            "feature_importance": feature_importance,
            "training_time": "è®­ç»ƒå®Œæˆ",
            "trained_at": pd.Timestamp.now().isoformat()
        }
        
        self.trainingLogUpdated.emit(f"ç»“æœæ•°æ®å‡†å¤‡å®Œæˆï¼Œç»˜å›¾æ•°æ®é•¿åº¦æ£€æŸ¥:")
        self.trainingLogUpdated.emit(f"  - actual_train: {len(plot_data.get('actual_train', []))}")
        self.trainingLogUpdated.emit(f"  - predicted_train: {len(plot_data.get('predicted_train', []))}")
        self.trainingLogUpdated.emit(f"  - actual_test: {len(plot_data.get('actual_test', []))}")
        self.trainingLogUpdated.emit(f"  - predicted_test: {len(plot_data.get('predicted_test', []))}")
        
        return result
    
    def _generate_plot_data(self, test_result):
        """ç”Ÿæˆç»˜å›¾æ•°æ®"""
        try:
            y_true = test_result.get('y_true', [])
            y_pred = test_result.get('y_pred', [])
            
            self.trainingLogUpdated.emit(f"ç”Ÿæˆç»˜å›¾æ•°æ® - æµ‹è¯•æ•°æ®é•¿åº¦: y_true={len(y_true)}, y_pred={len(y_pred)}")
            
            # è½¬æ¢æµ‹è¯•æ•°æ®ä¸ºåˆ—è¡¨æ ¼å¼
            test_actual = y_true.tolist() if hasattr(y_true, 'tolist') else (y_true if isinstance(y_true, list) else [])
            test_predicted = y_pred.tolist() if hasattr(y_pred, 'tolist') else (y_pred if isinstance(y_pred, list) else [])
            
            # è·å–è®­ç»ƒé›†é¢„æµ‹
            train_actual = []
            train_predicted = []
            
            if hasattr(self.predictor, 'X_train') and hasattr(self.predictor, 'y_train'):
                try:
                    self.trainingLogUpdated.emit(f"æ‰¾åˆ°è®­ç»ƒæ•°æ® - X_train: {self.predictor.X_train.shape}, y_train: {self.predictor.y_train.shape}")
                    
                    # ä½¿ç”¨é¢„æµ‹å™¨çš„æ‰¹é‡é¢„æµ‹æ–¹æ³•
                    train_pred = self.predictor._predict_batch(self.predictor.X_train)
                    
                    train_actual = self.predictor.y_train.tolist() if hasattr(self.predictor.y_train, 'tolist') else self.predictor.y_train
                    train_predicted = train_pred.tolist() if hasattr(train_pred, 'tolist') else train_pred
                    
                    self.trainingLogUpdated.emit(f"è®­ç»ƒæ•°æ®é•¿åº¦: actual={len(train_actual)}, predicted={len(train_predicted)}")
                    
                except Exception as e:
                    self.trainingLogUpdated.emit(f"è·å–è®­ç»ƒé›†é¢„æµ‹å¤±è´¥: {str(e)}")
                    train_actual = []
                    train_predicted = []
            else:
                self.trainingLogUpdated.emit("é¢„æµ‹å™¨ç¼ºå°‘ X_train æˆ– y_train å±æ€§")
            
            result = {
                "actual_train": train_actual,
                "predicted_train": train_predicted,
                "actual_test": test_actual,
                "predicted_test": test_predicted
            }
            
            # è®°å½•æœ€ç»ˆç»“æœ
            self.trainingLogUpdated.emit(f"ç»˜å›¾æ•°æ®ç”Ÿæˆå®Œæˆ: train_len={len(train_actual)}, test_len={len(test_actual)}")
            
            return result
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆç»˜å›¾æ•°æ®å¤±è´¥: {str(e)}"
            self.trainingLogUpdated.emit(error_msg)
            logger.exception(error_msg)
            return {"actual_train": [], "predicted_train": [], "actual_test": [], "predicted_test": []}
    
    def _calculate_feature_importance(self):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            if hasattr(self.predictor, 'model'):
                model = self.predictor.model
                
                if hasattr(model, 'feature_importances_'):
                    # éšæœºæ£®æ—ç­‰æ¨¡å‹
                    importance = model.feature_importances_
                elif hasattr(model, 'coef_'):
                    # çº¿æ€§æ¨¡å‹ã€SVRç­‰
                    importance = np.abs(model.coef_).flatten()
                else:
                    # ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§çš„æ¨¡å‹ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
                    feature_count = len(self.features)
                    importance = np.ones(feature_count) / feature_count
                
                return [
                    {"feature": feature, "importance": float(imp)}
                    for feature, imp in zip(self.features, importance)
                ]
            
            return []
            
        except Exception as e:
            self.trainingLogUpdated.emit(f"è®¡ç®—ç‰¹å¾é‡è¦æ€§å¤±è´¥: {str(e)}")
            return []
    
    def _generate_model_name(self):
        """ç”Ÿæˆæ¨¡å‹åç§°"""
        tables_str = "_".join(self.table_names[:2])
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        return f"model_{tables_str}_{timestamp}"
    
    def _save_model_info(self, result_data):
        """ä¿å­˜æ¨¡å‹ä¿¡æ¯åˆ°çº¿ç¨‹å¯¹è±¡"""
        self.model_info = {
            "model": self.predictor.model,
            "scaler": self.predictor.scaler,
            "model_instance": self.predictor,
            "task_type": self.task_type,
            "features": self.feature_mapping.values() if self.feature_mapping else self.features,
            "original_features": self.features,
            "feature_mapping": self.feature_mapping,
            "target": self.target_label,
            "type": self.predictor.model_info.model_type.upper(),
            "table_names": self.table_names,
            **{k: v for k, v in result_data.items() if k.startswith(('train_', 'test_'))},
            "r2_plot_data": result_data.get("r2_plot_data", {}),
            "error_plot_data": result_data.get("error_plot_data", {}),
            "trained_at": result_data.get("trained_at")
        }


class ContinuousLearningController(QObject):
    """æŒç»­å­¦ä¹ æ§åˆ¶å™¨ - é‡æ„ä¸ºä½¿ç”¨ç»Ÿä¸€é¢„æµ‹å™¨æ¥å£"""
    
    # ä¿¡å·å®šä¹‰
    taskSelectionChanged = Signal(int)
    phaseChanged = Signal(str)
    predictionStarted = Signal(int)
    predictionCompleted = Signal(int, dict)
    predictionFailed = Signal(int, str)
    dataPreparationStarted = Signal(int)
    dataPreparationCompleted = Signal(int, dict)
    trainingStarted = Signal(int)
    trainingCompleted = Signal(str, dict)
    evaluationStarted = Signal(int)
    evaluationCompleted = Signal(int, dict)
    
    # æ·±åº¦å­¦ä¹ å¯è§†åŒ–ä¿¡å·
    lossDataUpdated = Signal(dict)
    trainingProgressUpdated = Signal(float, dict)
    trainingLogUpdated = Signal(str)
    
    # æ•°æ®ç®¡ç†ä¿¡å·
    dataListUpdated = Signal(list)
    dataLoaded = Signal(dict)
    dataAdded = Signal(dict)
    dataUpdated = Signal(dict)
    dataDeleted = Signal(int)
    
    # æ¨¡å‹ç®¡ç†ä¿¡å·
    modelListUpdated = Signal(list)
    trainingProgressChanged = Signal(float)
    trainingError = Signal(str)
    modelSaved = Signal(str, str)
    testResultsUpdated = Signal(dict)
    testProgressUpdated = Signal(float)
    testLogUpdated = Signal(str)
    
    # æ•°æ®åº“ä¿¡å·
    tablesListUpdated = Signal(list)
    fieldsListUpdated = Signal(list)
    
    def __init__(self):
        super().__init__()
        self._selected_task = -1
        self._current_phase = "task_selection"
        self._task_names = {
            0: {"zh": "æ‰¬ç¨‹é¢„æµ‹", "en": "Head Prediction"},
            1: {"zh": "äº§é‡é¢„æµ‹", "en": "Production Prediction"}, 
            2: {"zh": "æ°”æ¶²æ¯”é¢„æµ‹", "en": "Gas-Liquid Ratio Prediction"}
        }
        self._ml_service = None
        self._project_id = -1
        self._db_path = "data/oil_analysis.db"
        
        # è®­ç»ƒé…ç½® - ä½¿ç”¨æ–°çš„é…ç½®ç±»
        self._training_config = TrainingConfig(
            learning_rate=0.001,
            epochs=1000,
            batch_size=48,
            patience=100,
            test_size=0.2
        )
        
        # æ•°æ®ç®¡ç†
        self._training_data = []
        self._test_data = []
        self._selected_tables = []
        self._selected_features = []
        self._target_label = ""
        
        # æ¨¡å‹ç®¡ç† - ä½¿ç”¨ç»Ÿä¸€çš„é¢„æµ‹å™¨æ¥å£
        self._predictors = {}  # å­˜å‚¨é¢„æµ‹å™¨å®ä¾‹
        self._models = {}      # å­˜å‚¨æ¨¡å‹ä¿¡æ¯
        self._current_model = None
        self._training_history = {}
        self._test_results = {}
        
        # Excelæ–‡ä»¶è·¯å¾„
        self._excel_file_path = ""
        
        # è®­ç»ƒçº¿ç¨‹
        self._training_thread = None
        self._mutex = QMutex()
    
    # ================== è®­ç»ƒå‚æ•°è®¾ç½®åŠŸèƒ½ ==================
    
    @Slot(float, int, int, int)
    def setTrainingParams(self, learning_rate, epochs, batch_size, patience):
        """è®¾ç½®è®­ç»ƒå‚æ•° - ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿ"""
        self._training_config.learning_rate = learning_rate
        self._training_config.epochs = epochs
        self._training_config.batch_size = batch_size
        self._training_config.patience = patience
        
        logger.info(f"è®­ç»ƒå‚æ•°å·²æ›´æ–°: lr={learning_rate}, epochs={epochs}, batch_size={batch_size}, patience={patience}")
    
    @Slot(result='QVariant')
    def getTrainingParams(self):
        """è·å–å½“å‰è®­ç»ƒå‚æ•°"""
        return {
            'learning_rate': self._training_config.learning_rate,
            'epochs': self._training_config.epochs,
            'batch_size': self._training_config.batch_size,
            'patience': self._training_config.patience
        }
        
    @Slot(str, result=list)
    def getModelExpectedFeatures(self, task_type):
        """è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„æ¨¡å‹æœŸæœ›ç‰¹å¾"""
        try:
            return ModelFeatureConfig.get_expected_features(task_type)
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹æœŸæœ›ç‰¹å¾å¤±è´¥: {str(e)}")
            return []
    
    @Slot(str, result=list)
    def getModelExpectedTargets(self, task_type):
        """è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„å¯èƒ½ç›®æ ‡å˜é‡"""
        try:
            return ModelFeatureConfig.get_expected_targets(task_type)
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹æœŸæœ›ç›®æ ‡å¤±è´¥: {str(e)}")
            return []
    
    @Slot(result=list)
    def getAllSupportedTasks(self):
        """è·å–æ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡ç±»å‹"""
        try:
            return ModelFeatureConfig.get_all_tasks()
        except Exception as e:
            logger.error(f"è·å–æ”¯æŒçš„ä»»åŠ¡ç±»å‹å¤±è´¥: {str(e)}")
            return []
    
    # ================== ä»»åŠ¡é€‰æ‹©åŠŸèƒ½ ==================
    
    @Slot(int)
    def setSelectedTask(self, task_id):
        """è®¾ç½®é€‰æ‹©çš„ä»»åŠ¡"""
        if task_id in self._task_names:
            self._selected_task = task_id
            self.taskSelectionChanged.emit(task_id)
            logger.info(f"é€‰æ‹©ä»»åŠ¡: {self._task_names[task_id]['zh']}")
    
    @Slot(result=int)
    def getSelectedTask(self):
        """è·å–å½“å‰é€‰æ‹©çš„ä»»åŠ¡"""
        return self._selected_task
    
    @Slot(result=list)
    def getTaskList(self):
        """è·å–ä»»åŠ¡åˆ—è¡¨"""
        return [
            {"id": 0, "name": "æ‰¬ç¨‹é¢„æµ‹", "name_en": "Head Prediction"},
            {"id": 1, "name": "äº§é‡é¢„æµ‹", "name_en": "Production Prediction"},
            {"id": 2, "name": "æ°”æ¶²æ¯”é¢„æµ‹", "name_en": "Gas-Liquid Ratio Prediction"}
        ]
    
    # ================== æ¨¡å‹è®­ç»ƒåŠŸèƒ½ - ä½¿ç”¨ç»Ÿä¸€æ¥å£ ==================
    
    @Slot(int, list, list, str, str, dict)
    def startModelTrainingWithData(self, project_id, table_names, features, target_label, task_type="", feature_mapping=None):
        """ä½¿ç”¨æŒ‡å®šæ•°æ®å¼€å§‹æ¨¡å‹è®­ç»ƒ - ä½¿ç”¨ç»Ÿä¸€é¢„æµ‹å™¨æ¥å£"""
        try:
            # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„è®­ç»ƒçº¿ç¨‹ï¼Œå…ˆåœæ­¢å®ƒ
            if self._training_thread and self._training_thread.isRunning():
                self._training_thread.quit()
                self._training_thread.wait()
            
            # å‡†å¤‡è®­ç»ƒå‚æ•°
            training_params = {
                'learning_rate': self._training_config.learning_rate,
                'epochs': self._training_config.epochs,
                'batch_size': self._training_config.batch_size,
                'patience': self._training_config.patience
            }
            
            # åˆ›å»ºæ–°çš„è®­ç»ƒçº¿ç¨‹
            self._training_thread = ModelTrainingThread(
                project_id, table_names, features, target_label, task_type, 
                self._db_path, feature_mapping, training_params
            )
            
            # è¿æ¥ä¿¡å·
            self._training_thread.trainingProgressUpdated.connect(self.trainingProgressUpdated.emit)
            self._training_thread.trainingCompleted.connect(self._on_training_completed)
            self._training_thread.trainingError.connect(self.trainingError.emit)
            self._training_thread.trainingLogUpdated.connect(self.trainingLogUpdated.emit)
            
            # è¿æ¥æŸå¤±æ•°æ®ä¿¡å·
            def on_loss_data_relay(loss_data):
                logger.info(f"=== ContinuousLearningController: ä¸­ç»§æŸå¤±æ•°æ® ===")
                logger.info(f"æ¥æ”¶åˆ°çš„æŸå¤±æ•°æ®: {loss_data}")
                try:
                    self.lossDataUpdated.emit(loss_data)
                    logger.info("æŸå¤±æ•°æ®ä¿¡å·å‘å°„åˆ°UIæˆåŠŸ")
                except Exception as e:
                    logger.error(f"å‘å°„æŸå¤±æ•°æ®ä¿¡å·åˆ°UIå¤±è´¥: {e}")
            
            self._training_thread.lossDataUpdated.connect(on_loss_data_relay)
            
            # å¯åŠ¨è®­ç»ƒ
            self.trainingStarted.emit(project_id)
            self._training_thread.start()
            
        except Exception as e:
            error_msg = f"å¯åŠ¨æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            self.trainingError.emit(error_msg)
    
    def _on_training_completed(self, model_name, result):
        """è®­ç»ƒå®Œæˆå›è°ƒ - ä½¿ç”¨ç»Ÿä¸€æ¥å£"""
        try:
            if self._training_thread and self._training_thread.model_info:
                # ä¿å­˜é¢„æµ‹å™¨å®ä¾‹
                model_name_str = str(model_name)
                self._predictors[model_name_str] = self._training_thread.predictor
                self._models[model_name_str] = self._training_thread.model_info
                self._current_model = model_name_str
                
                logger.info(f"è®¾ç½® _current_model = {self._current_model}")
                
                # å‘é€å®Œæˆä¿¡å·
                self.trainingCompleted.emit(model_name_str, result)
                self.modelListUpdated.emit(list(self._models.keys()))
                
                logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_name_str}")
            else:
                logger.warning("è®­ç»ƒçº¿ç¨‹æˆ–æ¨¡å‹ä¿¡æ¯ä¸ºç©º")
            
        except Exception as e:
            error_msg = f"è®­ç»ƒå®Œæˆå¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            self.trainingError.emit(error_msg)
    
    # ================== æ¨¡å‹ç®¡ç†åŠŸèƒ½ - ä½¿ç”¨ç»Ÿä¸€æ¥å£ ==================
    
    @Slot(result=list)
    def getAvailableModels(self):
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return list(self._models.keys())
    
    @Slot(str, result=dict)
    def getModelInfo(self, model_name):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_name in self._models:
            model_info = self._models[model_name].copy()
            # ç§»é™¤ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
            model_info.pop('model', None)
            model_info.pop('scaler', None)
            model_info.pop('model_instance', None)
            return model_info
        return {}
    
    @Slot(str, result=str)
    def saveModelWithDialog(self, model_name):
        """é€šè¿‡å¯¹è¯æ¡†ä¿å­˜æ¨¡å‹ - ä½¿ç”¨ç»Ÿä¸€æ¥å£"""
        try:
            if model_name not in self._predictors:
                raise ValueError("é¢„æµ‹å™¨ä¸å­˜åœ¨")
            
            predictor = self._predictors[model_name]
            model_info = self._models[model_name]
            task_type = model_info.get('task_type', 'unknown')
            
            # è®¾ç½®é»˜è®¤è·¯å¾„
            default_base_path = Path(__file__).parent.parent
            if task_type == "head":
                default_save_dir = default_base_path / "TDHsave"
                task_name = "TDH"
            elif task_type == "production":
                default_save_dir = default_base_path / "QFsave"
                task_name = "QF"
            elif task_type == "glr":
                default_save_dir = default_base_path / "GLRsave"
                task_name = "GLR"
            else:
                default_save_dir = default_base_path / "saved_models"
                task_name = "MODEL"
            
            # ç¡®ä¿é»˜è®¤ç›®å½•å­˜åœ¨
            default_save_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å¤¹å
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            model_folder_name = f"{task_name}-{timestamp}"
            default_full_path = default_save_dir / model_folder_name
            
            # ä½¿ç”¨æ–‡ä»¶å¯¹è¯æ¡†
            from PySide6.QtWidgets import QFileDialog
            save_path, _ = QFileDialog.getSaveFileName(
                None,
                f"ä¿å­˜ {task_name} æ¨¡å‹",
                str(default_full_path),
                "æ¨¡å‹æ–‡ä»¶å¤¹ (*);;æ‰€æœ‰æ–‡ä»¶ (*)"
            )
            
            if save_path:
                # ä½¿ç”¨é¢„æµ‹å™¨çš„ç»Ÿä¸€ä¿å­˜æ¥å£
                save_path_obj = Path(save_path)
                model_folder_name = save_path_obj.name
                
                success = predictor.save_model(model_folder_name)
                
                if success:
                    # è®¡ç®—å®é™…ä¿å­˜è·¯å¾„
                    if task_type == "head":
                        actual_save_path = default_base_path / "TDHsave" / model_folder_name
                    elif task_type == "production":
                        actual_save_path = default_base_path / "QFsave" / model_folder_name
                    elif task_type == "glr":
                        actual_save_path = default_base_path / "GLRsave" / model_folder_name
                    else:
                        actual_save_path = save_path_obj
                    
                    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {actual_save_path}")
                    self.modelSaved.emit(model_name, str(actual_save_path))
                    return str(actual_save_path)
                else:
                    logger.error("æ¨¡å‹ä¿å­˜å¤±è´¥")
                    return ""
            
            return ""  # ç”¨æˆ·å–æ¶ˆä¿å­˜
            
        except Exception as e:
            error_msg = f"ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return ""
    
    @Slot(str, str, result=str)
    def saveModelWithCustomName(self, model_name, custom_name):
        """ä½¿ç”¨è‡ªå®šä¹‰åç§°ä¿å­˜æ¨¡å‹ - ä½¿ç”¨ç»Ÿä¸€æ¥å£"""
        try:
            if model_name not in self._predictors:
                logger.warning(f"é¢„æµ‹å™¨ {model_name} ä¸å­˜åœ¨")
                return ""
            
            predictor = self._predictors[model_name]
            model_info = self._models[model_name]
            task_type = model_info.get('task_type', 'unknown')
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©é»˜è®¤ä¿å­˜ç›®å½•
            default_base_path = Path(__file__).parent.parent
            if task_type == "head":
                task_name = "TDH"
            elif task_type == "production":
                task_name = "QF"
            elif task_type == "glr":
                task_name = "GLR"
            else:
                task_name = "MODEL"
            
            # ç”Ÿæˆæœ€ç»ˆåç§°
            final_name = f"{task_name}-{custom_name}" if custom_name else f"{task_name}-{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}"
            
            # ä½¿ç”¨é¢„æµ‹å™¨çš„ç»Ÿä¸€ä¿å­˜æ¥å£
            success = predictor.save_model(final_name)
            
            if success:
                # è®¡ç®—å®é™…ä¿å­˜è·¯å¾„
                if task_type == "head":
                    actual_save_path = default_base_path / "TDHsave" / final_name
                elif task_type == "production":
                    actual_save_path = default_base_path / "QFsave" / final_name
                elif task_type == "glr":
                    actual_save_path = default_base_path / "GLRsave" / final_name
                else:
                    actual_save_path = default_base_path / "saved_models" / final_name
                
                logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {actual_save_path}")
                self.modelSaved.emit(model_name, str(actual_save_path))
                return str(actual_save_path)
            else:
                logger.error("æ¨¡å‹ä¿å­˜å¤±è´¥")
                return ""
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨è‡ªå®šä¹‰åç§°ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return ""
    
    @Slot(result=str)
    def getCurrentModelName(self):
        """è·å–å½“å‰è®­ç»ƒçš„æ¨¡å‹åç§°"""
        current_model = self._current_model if self._current_model else ""
        logger.info(f"getCurrentModelName è¢«è°ƒç”¨ï¼Œè¿”å›: {current_model}")
        return current_model
    
    # ================== æ¨¡å‹æµ‹è¯•åŠŸèƒ½ - ä½¿ç”¨ç»Ÿä¸€æ¥å£ ==================
    
    @Slot(str, str, list, list, str, dict)
    def startModelTestingWithConfiguration(self, model_path, model_type, data_tables, features, target_label, feature_mapping):
        """ä½¿ç”¨å®Œæ•´é…ç½®å¼€å§‹æ¨¡å‹æµ‹è¯• - ä½¿ç”¨ç»Ÿä¸€æ¥å£"""
        try:
            self.testLogUpdated.emit("å¼€å§‹æ¨¡å‹æµ‹è¯•...")
            self.testProgressUpdated.emit(0.0)
            print(f"å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_path}, ç±»å‹: {model_type}")
            # åŠ è½½å¤–éƒ¨æ¨¡å‹ - åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
            predictor = self._load_external_predictor(model_path, model_type)
            if predictor is None:
                error_msg = f"æ— æ³•åŠ è½½æ¨¡å‹: {model_path},{model_type}"
                self.testLogUpdated.emit(error_msg)
                return
            
            self.testProgressUpdated.emit(20.0)
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            X_test, y_test = self._load_test_data(data_tables, features, target_label, feature_mapping)
            if X_test is None or y_test is None:
                error_msg = "æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥"
                self.testLogUpdated.emit(error_msg)
                return
            
            self.testProgressUpdated.emit(60.0)
            
            # ä½¿ç”¨é¢„æµ‹å™¨çš„ç»Ÿä¸€æµ‹è¯•æ¥å£
            self.testLogUpdated.emit("å¼€å§‹é¢„æµ‹...")
            
            # æ‰‹åŠ¨è®¾ç½®æµ‹è¯•æ•°æ®åˆ°é¢„æµ‹å™¨
            predictor.X_test = X_test
            predictor.y_test = y_test
            predictor.is_trained = True
            
            # æ‰§è¡Œæµ‹è¯•
            test_result = predictor.test()
            
            self.testProgressUpdated.emit(90.0)
            
            # å‡†å¤‡ç»“æœæ•°æ®
            test_results = {
                "model_path": model_path,
                "model_type": model_type,
                "test_tables": data_tables,
                "features": features,
                "target": target_label,
                "feature_mapping": feature_mapping,
                "test_samples": len(y_test),
                "tested_at": pd.Timestamp.now().isoformat(),
                **test_result['metrics'],
                "error_plot_data": {
                    "actual": test_result['y_true'].tolist(),
                    "predicted": test_result['y_pred'].tolist()
                }
            }
            
            self.testProgressUpdated.emit(100.0)
            self.testLogUpdated.emit(f"æµ‹è¯•å®Œæˆ! MAPE: {test_results.get('mape', 0):.2f}%, RÂ²: {test_results.get('r2', 0):.4f}")
            
            # å‘é€æµ‹è¯•ç»“æœ
            self.testResultsUpdated.emit(test_results)
            
        except Exception as e:
            error_msg = f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            self.testLogUpdated.emit(error_msg)
    
    def _load_external_predictor(self, model_path, model_type):
        """åŠ è½½å¤–éƒ¨é¢„æµ‹å™¨ - ä½¿ç”¨ç»Ÿä¸€æ¥å£"""
        try:
            path_obj = Path(model_path)
            # æ ¹æ®æ¨¡å‹ç±»å‹æˆ–è·¯å¾„ç‰¹å¾åˆ¤æ–­ä½¿ç”¨å“ªä¸ªé¢„æµ‹å™¨
            if "GLR" in model_type.upper() or "glr" in model_path.lower():
                predictor = GLRPredictor([], [])
                success = predictor.load_model(model_path)
                if success:
                    self.testLogUpdated.emit("GLRé¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
                    return predictor
                    
            elif "TDH" in model_type.upper() or "tdh" in model_path.lower() or "head" in model_path.lower():
                predictor = TDHPredictor([], [])
                success = predictor.load_model(model_path)
                if success:
                    self.testLogUpdated.emit("TDHé¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
                    return predictor
                    
            elif "QF" in model_type.upper() or "qf" in model_path.lower() or "production" in model_path.lower():
                predictor = QFPredictor([], [])
                success = predictor.load_model(model_path)
                if success:
                    self.testLogUpdated.emit("QFé¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
                    return predictor
            
            self.testLogUpdated.emit(f"æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹: {model_type}")
            return None
            
        except Exception as e:
            self.testLogUpdated.emit(f"åŠ è½½é¢„æµ‹å™¨å¤±è´¥: {str(e)}")
            return None
    
    def _load_test_data(self, data_tables, features, target_label, feature_mapping):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            # åˆå¹¶æ‰€æœ‰æ•°æ®è¡¨
            all_data = []
            for table_name in data_tables:
                try:
                    conn = sqlite3.connect(self._db_path)
                    df = pd.read_sql_query(f"SELECT * FROM \"{table_name}\"", conn)
                    conn.close()
                    all_data.append(df)
                    self.testLogUpdated.emit(f"å·²åŠ è½½è¡¨ {table_name}: {len(df)} è¡Œ")
                except Exception as e:
                    self.testLogUpdated.emit(f"åŠ è½½è¡¨ {table_name} å¤±è´¥: {str(e)}")
                    continue
            
            if not all_data:
                self.testLogUpdated.emit("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®è¡¨")
                return None, None
            
            # åˆå¹¶æ•°æ®
            combined_df = pd.concat(all_data, ignore_index=True)
            self.testLogUpdated.emit(f"åˆå¹¶æ•°æ®å®Œæˆ: æ€»å…± {len(combined_df)} è¡Œ")
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾æ˜ å°„é€»è¾‘
            if feature_mapping:
                # æŒ‰ç…§ç‰¹å¾æ˜ å°„çš„é¡ºåºæ„å»ºæµ‹è¯•ç‰¹å¾
                mapped_features = []
                self.testLogUpdated.emit("åº”ç”¨ç‰¹å¾æ˜ å°„:")
                for model_feature, user_feature in feature_mapping.items():
                    if user_feature and user_feature.strip():
                        mapped_features.append(user_feature)
                        self.testLogUpdated.emit(f"  æ¨¡å‹ç‰¹å¾ {model_feature} â†’ ç”¨æˆ·æ•°æ® {user_feature}")
                
                if not mapped_features:
                    self.testLogUpdated.emit("ç‰¹å¾æ˜ å°„ä¸ºç©ºï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç‰¹å¾")
                    mapped_features = features
            else:
                mapped_features = features
                self.testLogUpdated.emit("æœªä½¿ç”¨ç‰¹å¾æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç‰¹å¾")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_cols = mapped_features + [target_label]
            missing_cols = [col for col in required_cols if col not in combined_df.columns]
            if missing_cols:
                self.testLogUpdated.emit(f"æ•°æ®ä¸­ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
                return None, None
            
            # ğŸ”¥ ä½¿ç”¨æ˜ å°„åçš„ç‰¹å¾é¡ºåº
            self.testLogUpdated.emit(f"æœ€ç»ˆç‰¹å¾é¡ºåº: {mapped_features}")
            X_test = combined_df[mapped_features].values
            y_test = combined_df[target_label].values
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            valid_indices = ~(np.isnan(X_test).any(axis=1) | np.isnan(y_test))
            X_test = X_test[valid_indices]
            y_test = y_test[valid_indices]
            
            self.testLogUpdated.emit(f"æœ‰æ•ˆæµ‹è¯•æ ·æœ¬: {len(X_test)} ä¸ª")
            
            if len(X_test) == 0:
                self.testLogUpdated.emit("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ ·æœ¬")
                return None, None
            
            return X_test, y_test
            
        except Exception as e:
            self.testLogUpdated.emit(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}")
            return None, None
    
    # ================== å…¶ä»–åŠŸèƒ½ä¿æŒä¸å˜ ==================
    
    # Excelæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
    @Slot(str)
    def setDataFilePath(self, file_path):
        """è®¾ç½®æ•°æ®æ–‡ä»¶è·¯å¾„"""
        self._excel_file_path = file_path
        logger.info(f"è®¾ç½®æ•°æ®æ–‡ä»¶è·¯å¾„: {file_path}")
    
    @Slot(result=dict)
    @Slot(str, result=dict)
    def uploadDataFileToDatabase(self, custom_table_name=""):
        """ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ°æ•°æ®åº“"""
        try:
            if not self._excel_file_path:
                return {"success": False, "error": "æœªé€‰æ‹©æ•°æ®æ–‡ä»¶"}
            
            file_path = self._excel_file_path
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
            if file_path.lower().endswith('.csv'):
                df = pd.read_csv(file_path)
            elif file_path.lower().endswith(('.xlsx', '.xls')):
                df = pd.read_excel(file_path)
            else:
                return {"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·é€‰æ‹©Excel(.xlsx/.xls)æˆ–CSV(.csv)æ–‡ä»¶"}
            
            # ç¡®å®šè¡¨å
            if custom_table_name.strip():
                table_name = custom_table_name.strip()
            else:
                timestamp = int(time.time())
                table_name = f"data_upload_{timestamp}"
            
            # è¿æ¥æ•°æ®åº“
            conn = sqlite3.connect(self._db_path)
            df.to_sql(table_name, conn, if_exists='replace', index=False)
            conn.close()
            
            logger.info(f"æ•°æ®æ–‡ä»¶æˆåŠŸä¸Šä¼ åˆ°è¡¨: {table_name}")
            self.tablesListUpdated.emit(self.getAvailableTables())
            
            return {
                "success": True, 
                "table_name": table_name,
                "records": len(df),
                "columns": list(df.columns)
            }
            
        except Exception as e:
            error_msg = f"æ•°æ®æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return {"success": False, "error": error_msg}
    
    # æ•°æ®åº“ç®¡ç†åŠŸèƒ½
    @Slot(result=list)
    def getAvailableTables(self):
        """è·å–æ•°æ®åº“ä¸­çš„å¯ç”¨è¡¨"""
        try:
            conn = sqlite3.connect(self._db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            all_tables = [row[0] for row in cursor.fetchall()]
            conn.close()
            
            # è¿‡æ»¤åªä¿ç•™dataæˆ–testå¼€å¤´çš„è¡¨
            filtered_tables = [table for table in all_tables if table.startswith('data') or table.startswith('test')]
            
            logger.info(f"Found {len(filtered_tables)} filtered tables: {filtered_tables}")
            self.tablesListUpdated.emit(filtered_tables)
            return filtered_tables
        except Exception as e:
            logger.error(f"è·å–æ•°æ®è¡¨å¤±è´¥: {str(e)}")
            return []
    
    @Slot(str, result=list)
    def getTableFields(self, table_name):
        """è·å–æŒ‡å®šè¡¨çš„å­—æ®µ"""
        try:
            conn = sqlite3.connect(self._db_path)
            cursor = conn.cursor()
            cursor.execute(f"PRAGMA table_info({table_name})")
            fields = [row[1] for row in cursor.fetchall()]
            conn.close()
            
            self.fieldsListUpdated.emit(fields)
            return fields
        except Exception as e:
            logger.error(f"è·å–è¡¨å­—æ®µå¤±è´¥: {str(e)}")
            return []
    
    @Slot(str, result=list)
    def getModelExpectedFeatures(self, task_type):
        """è·å–æŒ‡å®šä»»åŠ¡ç±»å‹çš„æ¨¡å‹æœŸæœ›ç‰¹å¾"""
        try:
            return ModelFeatureConfig.get_expected_features(task_type)
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹æœŸæœ›ç‰¹å¾å¤±è´¥: {str(e)}")
            return []
    
    @Slot(str, result='QVariant')
    def previewTableData(self, table_name):
        """é¢„è§ˆè¡¨æ•°æ®"""
        try:
            conn = sqlite3.connect(self._db_path)
            cursor = conn.cursor()
            
            # è·å–å‰20è¡Œæ•°æ®
            cursor.execute(f"SELECT * FROM \"{table_name}\" LIMIT 20")
            rows = cursor.fetchall()
            
            # è·å–åˆ—å
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = [row[1] for row in cursor.fetchall()]
            
            conn.close()
            
            if not rows:
                return {
                    "success": True,
                    "columns": [],
                    "rows": [],
                    "message": "è¡¨ä¸ºç©º"
                }
            
            # æ ¼å¼åŒ–æ•°æ®ä¸ºäºŒç»´æ•°ç»„
            formatted_rows = []
            for row in rows:
                formatted_row = [str(val) if val is not None else "NULL" for val in row]
                formatted_rows.append(formatted_row)
            
            return {
                "success": True,
                "columns": columns,
                "rows": formatted_rows,
                "total_rows": len(rows)
            }
            
        except Exception as e:
            error_msg = f"é¢„è§ˆè¡¨æ•°æ®å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg
            }
    
    @Slot(result=list)
    def getTrainingDataList(self):
        """è·å–è®­ç»ƒæ•°æ®åˆ—è¡¨"""
        try:
            if hasattr(self, '_training_data') and self._training_data is not None:
                # ç¡®ä¿ _training_data æ˜¯ DataFrame å¯¹è±¡
                if isinstance(self._training_data, pd.DataFrame):
                    data_list = self._training_data.head(100).to_dict('records')  # åªè¿”å›å‰100æ¡ç”¨äºæ˜¾ç¤º
                    return data_list
                elif isinstance(self._training_data, list):
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›å‰100ä¸ªå…ƒç´ 
                    return self._training_data[:100]
            return []
        except Exception as e:
            logger.error(f"è·å–è®­ç»ƒæ•°æ®åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
        
    
    @Slot(str, result='QVariant')
    def deleteTable(self, table_name):
        """åˆ é™¤æ•°æ®è¡¨"""
        try:
            conn = sqlite3.connect(self._db_path)
            cursor = conn.cursor()
            
            # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table_name,))
            exists = cursor.fetchone()
            if not exists:
                conn.close()
                return {
                    "success": False,
                    "error": f"è¡¨ {table_name} ä¸å­˜åœ¨"
                }
            
            cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
            conn.commit()
            conn.close()
            
            logger.info(f"æˆåŠŸåˆ é™¤è¡¨: {table_name}")
            return {
                "success": True,
                "message": f"è¡¨ {table_name} å·²åˆ é™¤"
            }
            
        except Exception as e:
            error_msg = f"åˆ é™¤è¡¨å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg
            }
    
    # Property å®šä¹‰
    @Property(str, notify=phaseChanged)
    def currentPhase(self):
        return self._current_phase
        
    @currentPhase.setter
    def currentPhase(self, phase):
        if self._current_phase != phase:
            self._current_phase = phase
            self.phaseChanged.emit(phase)
            logger.info(f"é˜¶æ®µåˆ‡æ¢åˆ°: {phase}")
        
    @Property(int, notify=taskSelectionChanged)
    def selectedTask(self):
        return self._selected_task
        
    @selectedTask.setter 
    def selectedTask(self, task_id):
        if self._selected_task != task_id:
            self._selected_task = task_id
            self.taskSelectionChanged.emit(task_id)